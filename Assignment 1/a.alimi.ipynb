{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 Machine Learning Assignment 1 - Instructions & Guidelines\n",
    "\n",
    "### **📝 General Guidelines**\n",
    "Welcome to Machine Learning Assignment 1! This assignment will test your understanding of **regression and classification models**, including **data preprocessing, hyperparameter tuning, and model evaluation**.\n",
    "\n",
    "Follow the instructions carefully, and ensure your implementation is **correct, well-structured, and efficient**.\n",
    "\n",
    "🔹 **Submission Format:**  \n",
    "- Your submission **must be a single Jupyter Notebook (.ipynb)** file.  \n",
    "- **File Naming Convention:**  \n",
    "  - Use **your university email as the filename**, e.g.,  \n",
    "    ```\n",
    "    j.doe@innopolis.university.ipynb\n",
    "    ```\n",
    "  - **Do NOT modify this format**, or your submission may not be graded.\n",
    "\n",
    "🔹 **Assignment Breakdown:**\n",
    "| Task | Description | Points |\n",
    "|------|------------|--------|\n",
    "| **Task 1.1** | Linear Regression | 20 |\n",
    "| **Task 1.2** | Polynomial Regression | 20 |\n",
    "| **Task 2.1** | Data Preprocessing | 15 |\n",
    "| **Task 2.2** | Model Comparison | 45 |\n",
    "| **Total** | - | **100** |\n",
    "\n",
    "---\n",
    "\n",
    "### **📂 Dataset & Assumptions**\n",
    "The dataset files are stored in the `datasets/` folder.  \n",
    "- **Regression Dataset:** `datasets/task1_data.csv`\n",
    "- **Classification Dataset:** `datasets/pokemon_modified.csv`\n",
    "\n",
    "Each dataset is structured as follows:\n",
    "\n",
    "🔹 **`task1_data.csv` (for regression tasks)**  \n",
    "- Contains `X_train`, `y_train`, `X_test`, and `y_test`.  \n",
    "- The goal is to fit **linear and polynomial regression models** and evaluate their performance.  \n",
    "\n",
    "🔹 **`pokemon_modified.csv` (for classification tasks)**  \n",
    "- Contains Pokémon attributes, with `is_legendary` as the **binary target variable (0 or 1)**.  \n",
    "- Some features contain **missing values** and **categorical variables**, requiring preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "### **🚀 How to Approach the Assignment**\n",
    "1. **Start with Regression (Task 1)**\n",
    "   - Implement **linear regression** and **polynomial regression**.\n",
    "   - Use **GridSearchCV** for polynomial regression to find the best degree.\n",
    "   - Evaluate using **MSE, RMSE, MAE, and R² Score**.\n",
    "\n",
    "2. **Move to Data Preprocessing (Task 2.1)**\n",
    "   - Load and clean the Pokémon dataset.\n",
    "   - Handle **missing values** correctly.\n",
    "   - Encode categorical variables properly.\n",
    "   - Ensure **no data leakage** when doing the preprocessing.\n",
    "\n",
    "3. **Train and Evaluate Classification Models (Task 2.2)**\n",
    "   - Train **Logistic Regression, KNN, and Naive Bayes**.\n",
    "   - Use **GridSearchCV** for hyperparameter tuning.\n",
    "   - Evaluate models using **Accuracy, Precision, Recall, and F1-score**.\n",
    "\n",
    "---\n",
    "\n",
    "### **📌 Grading & Evaluation**\n",
    "- Your notebook will be **autograded**, so ensure:\n",
    "  - Your function names **exactly match** the given specifications.\n",
    "  - Your output format matches the expected results.\n",
    "- Partial credit will be given where applicable.\n",
    "\n",
    "🔹 **Need Help?**  \n",
    "- If you have any questions, refer to the **assignment markdown instructions** in each task before asking for clarifications.\n",
    "- You can post your question on this [Google sheet](https://docs.google.com/spreadsheets/d/1oyrqXDjT2CeGYx12aZhZ-oDKcQQ-PCgT91wHPhTlBCY/edit?usp=sharing)\n",
    "\n",
    "🚀 **Good luck! Happy coding!** 🎯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQ\n",
    "\n",
    "**1) Should we include the lines to import the libraries?**\n",
    "\n",
    "- **Answer:**  \n",
    "  It doesn't matter if you include extra import lines, as the grader will only call the specified functions.\n",
    "\n",
    "**2) Is it okay to submit my file with code outside of the functions?**\n",
    "\n",
    "- **Answer:**  \n",
    "  Yes, you can include additional code outside of the functions as long as the entire script runs correctly when converted to a `.py` file.\n",
    "\n",
    "**Important Clarification:**\n",
    "\n",
    "- The grader will first convert the Jupyter Notebook (.ipynb) into a Python file (.py) and then run it.\n",
    "- **Note:** Please do not include any commands like `!pip install numpy` because they may break the conversion process and therefore the submission will not be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear and Polynomial Regression (30 Points)\n",
    "\n",
    "### Task 1.1 - Linear Regression (15 Points)\n",
    "#### **Instructions**\n",
    "1. Load the dataset from **`datasets/task1_data.csv`**.\n",
    "2. Extract training and testing data from the following columns:\n",
    "   - `\"X_train\"`: Training feature values.\n",
    "   - `\"y_train\"`: Training target values.\n",
    "   - `\"X_test\"`: Testing feature values.\n",
    "   - `\"y_test\"`: Testing target values.\n",
    "3. Train a **linear regression model** on `X_train` and `y_train`.\n",
    "4. Use the trained model to predict `y_test` values.\n",
    "5. Compute and return the following **evaluation metrics** as a dictionary:\n",
    "   - **Mean Squared Error (MSE)**\n",
    "   - **Root Mean Squared Error (RMSE)**\n",
    "   - **Mean Absolute Error (MAE)**\n",
    "   - **R² Score**\n",
    "6. The function signature should match:\n",
    "   ```python\n",
    "   def task1_linear_regression() -> Dict[str, float]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please do not use any other libraries except for the ones imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import importlib.util\n",
    "import nbformat\n",
    "from tempfile import NamedTemporaryFile\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# Third-Party Library Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nbconvert import PythonExporter\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             mean_squared_error, mean_absolute_error, r2_score)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_linear_regression() -> Dict[str, float]:\n",
    "\n",
    "    file_path = \"datasets/task1_data.csv\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    X_train = data[\"X_train\"].values.reshape(-1, 1)\n",
    "    y_train = data[\"y_train\"].values\n",
    "    X_test = data[\"X_test\"].values.reshape(-1, 1)\n",
    "    y_test = data[\"y_test\"].values\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"R2\": r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 - Polynomial Regression (15 Points)\n",
    "\n",
    "#### **Instructions**\n",
    "1. Load the dataset from **`datasets/task1_data.csv`**.\n",
    "2. Extract training and testing data from the following columns:\n",
    "   - `\"X_train\"`: Training feature values.\n",
    "   - `\"y_train\"`: Training target values.\n",
    "   - `\"X_test\"`: Testing feature values.\n",
    "   - `\"y_test\"`: Testing target values.\n",
    "3. Define a **pipeline** that includes:\n",
    "   - **Polynomial feature transformation** (degree range: **2 to 10**).\n",
    "   - **Linear regression model**.\n",
    "4. Use **GridSearchCV** with **8-fold cross-validation** to determine the best polynomial degree.\n",
    "5. Train the model with the best polynomial degree and **evaluate it on the test set**.\n",
    "6. Compute and return the following results as a dictionary:\n",
    "   - **Best polynomial degree** (`best_degree`)\n",
    "   - **Mean Squared Error (MSE)**\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def task1_polynomial_regression() -> Dict[str, float]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_polynomial_regression() -> Dict[str, float]:\n",
    "    file_path = \"datasets/task1_data.csv\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    X_train = data[\"X_train\"].values.reshape(-1, 1)\n",
    "    y_train = data[\"y_train\"].values\n",
    "    X_test = data[\"X_test\"].values.reshape(-1, 1)\n",
    "    y_test = data[\"y_test\"].values\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('poly', PolynomialFeatures()),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    param_grid = {'poly__degree': list(range(2, 11))}\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=8, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_degree = grid_search.best_params_['poly__degree']\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"best_degree\": best_degree,\n",
    "        \"MSE\": mse\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Classification with Data Preprocessing (70 Points)\n",
    "\n",
    "### Task 2.1 - Data Preprocessing (30 Points)\n",
    "\n",
    "#### **Instructions**\n",
    "1. Load the dataset from **`datasets/pokemon_modified.csv`**.\n",
    "2. Look at the data and study the provided features\n",
    "3. Remove the **two redundant features**\n",
    "4. Handle **missing values**:\n",
    "   - Use **mean imputation** for **\"height_m\"** and **\"weight_kg\"**.\n",
    "   - Use **median imputation** for **\"percentage_male\"**.\n",
    "5. Perform **one-hot encoding** for the categorical column **\"type1\"**.\n",
    "6. Ensure the **target variable** (`\"is_legendary\"`) is present.\n",
    "7. **Split the data into training and testing sets** (`80%-20%` split). Is it balanced?\n",
    "8. **Apply feature scaling** using **StandardScaler** or **MinMaxScaler**.\n",
    "9. Return the following:\n",
    "   - `X_train_scaled`: Processed training features.\n",
    "   - `X_test_scaled`: Processed testing features.\n",
    "   - `y_train`: Training labels.\n",
    "   - `y_test`: Testing labels.\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def task2_preprocessing() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_preprocessing() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    file_path = \"datasets/pokemon_modified.csv\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    data = data.drop(columns=[\"name\", \"classification\"])\n",
    "    \n",
    "    X = data.drop(columns=[\"is_legendary\"])\n",
    "    y = data[\"is_legendary\"]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=91\n",
    "    )\n",
    "    \n",
    "    mean_features = [\"height_m\", \"weight_kg\"]\n",
    "    median_features = [\"percentage_male\"]\n",
    "    categorical_features = [\"type1\"]\n",
    "    \n",
    "    mean_imputer = SimpleImputer(strategy=\"mean\")\n",
    "    X_train[mean_features] = mean_imputer.fit_transform(X_train[mean_features])\n",
    "    X_test[mean_features] = mean_imputer.transform(X_test[mean_features])\n",
    "    \n",
    "    median_imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train[median_features] = median_imputer.fit_transform(X_train[median_features])\n",
    "    X_test[median_features] = median_imputer.transform(X_test[median_features])\n",
    "    \n",
    "    encoder = OneHotEncoder(drop=\"first\", sparse_output=False)\n",
    "    encoded_train = encoder.fit_transform(X_train[categorical_features])\n",
    "    encoded_test = encoder.transform(X_test[categorical_features])\n",
    "    encoded_cols = encoder.get_feature_names_out(categorical_features)\n",
    "    \n",
    "    X_train = pd.concat([\n",
    "        X_train.drop(columns=categorical_features).reset_index(drop=True),\n",
    "        pd.DataFrame(encoded_train, columns=encoded_cols)\n",
    "    ], axis=1)\n",
    "    \n",
    "    X_test = pd.concat([\n",
    "        X_test.drop(columns=categorical_features).reset_index(drop=True),\n",
    "        pd.DataFrame(encoded_test, columns=encoded_cols)\n",
    "    ], axis=1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 - Model Comparison (40 Points)\n",
    "\n",
    "#### **Instructions**\n",
    "1. **Train three classification models** on the preprocessed dataset:\n",
    "   - **Logistic Regression**\n",
    "   - **K-Nearest Neighbors (KNN)**\n",
    "   - **Gaussian Naive Bayes (GNB)**\n",
    "2. Use **GridSearchCV** for **hyperparameter tuning** on:\n",
    "   - **Logistic Regression**: Regularization strength (`C`) and penalty (`l1`, `l2`).\n",
    "   - **KNN**: Number of neighbors (`n_neighbors`), weight function, and distance metric.\n",
    "3. Train each model on the **training set** and evaluate on the **test set**.\n",
    "4. Compute the following **evaluation metrics**:\n",
    "   - **Accuracy**\n",
    "   - **Precision**\n",
    "   - **Recall**\n",
    "   - **F1 Score**\n",
    "5. Return a dictionary containing the evaluation metrics for each model.\n",
    "\n",
    "#### **Function Signature**\n",
    "```python\n",
    "def task2_model_comparison() -> Dict[str, Dict[str, float]]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_model_comparison() -> Dict[str, Dict[str, float]]:\n",
    "\n",
    "    X_train, X_test, y_train, y_test = task2_preprocessing()\n",
    "    \n",
    "    param_grid_logreg = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
    "    param_grid_knn = {'n_neighbors': range(1, 11), 'weights': ['uniform', 'distance'], 'p': [1, 2]}\n",
    "    \n",
    "    logreg = GridSearchCV(LogisticRegression(), param_grid_logreg, cv=5)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    logreg_best = logreg.best_estimator_\n",
    "    \n",
    "    knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_best = knn.best_estimator_\n",
    "    \n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    \n",
    "    models = {\"Logistic Regression\": logreg_best, \"KNN\": knn_best, \"Naive Bayes\": nb}\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        results[name] = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall\": recall_score(y_test, y_pred),\n",
    "            \"f1_score\": f1_score(y_test, y_pred)\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
